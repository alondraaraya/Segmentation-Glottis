{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7LH6KRi1SAI",
    "outputId": "4cba1293-281e-44e1-c733-84859297cc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "Successfully installed pillow-10.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 18:14:34.160549: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-28 18:14:34.181046: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-28 18:14:34.181068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-28 18:14:34.181664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-28 18:14:34.185297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 18:14:34.567937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow está utilizando la GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 18:14:34.839134: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-28 18:14:34.859137: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-28 18:14:34.859236: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs disponibles:\", physical_devices)\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"TensorFlow está utilizando la GPU.\")\n",
    "else:\n",
    "    print(\"TensorFlow no está utilizando la GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJOAEuuus9Uy",
    "outputId": "6f737d49-155f-4c80-836d-34dcfdb3ad2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 10:57:38.023920: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-12 10:57:38.392789: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 10:57:38.392900: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 10:57:38.448456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-12 10:57:38.551111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 10:57:39.104819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-12 10:57:39.911807: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.040854: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.041225: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.046536: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.046819: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.047037: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.106619: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.106691: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.106741: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-12 10:57:40.106784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13818 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-06-12 10:57:40.430429: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 10, 256, 256, 3)]    0         []                            \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 10, 256, 256, 64)     1792      ['input_1[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 10, 256, 256, 64)     36928     ['time_distributed[0][0]']    \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 10, 256, 256, 128)    590336    ['time_distributed_1[0][0]']  \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDi  (None, 10, 128, 128, 128)    0         ['bidirectional[0][0]']       \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDi  (None, 10, 128, 128, 128)    147584    ['time_distributed_2[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDi  (None, 10, 128, 128, 128)    147584    ['time_distributed_3[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 10, 128, 128, 256)    2360320   ['time_distributed_4[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDi  (None, 10, 64, 64, 256)      0         ['bidirectional_1[0][0]']     \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDi  (None, 10, 64, 64, 256)      590080    ['time_distributed_5[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_7 (TimeDi  (None, 10, 64, 64, 256)      590080    ['time_distributed_6[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  (None, 10, 64, 64, 128)      1475072   ['time_distributed_7[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDi  (None, 10, 128, 128, 128)    0         ['bidirectional_2[0][0]']     \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 10, 128, 128, 384)    0         ['time_distributed_8[0][0]',  \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_9 (TimeDi  (None, 10, 128, 128, 128)    442496    ['concatenate[0][0]']         \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_10 (TimeD  (None, 10, 128, 128, 128)    147584    ['time_distributed_9[0][0]']  \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirecti  (None, 10, 128, 128, 128)    885248    ['time_distributed_10[0][0]'] \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " time_distributed_11 (TimeD  (None, 10, 256, 256, 128)    0         ['bidirectional_3[0][0]']     \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 10, 256, 256, 256)    0         ['time_distributed_11[0][0]', \n",
      " )                                                                   'bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed_12 (TimeD  (None, 10, 256, 256, 64)     147520    ['concatenate_1[0][0]']       \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_13 (TimeD  (None, 10, 256, 256, 64)     36928     ['time_distributed_12[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirecti  (None, 10, 256, 256, 128)    590336    ['time_distributed_13[0][0]'] \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " time_distributed_14 (TimeD  (None, 10, 256, 256, 4)      516       ['bidirectional_4[0][0]']     \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8190404 (31.24 MB)\n",
      "Trainable params: 8190404 (31.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.layers import TimeDistributed, Bidirectional, LSTM, ConvLSTM2D\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    x = TimeDistributed(Conv2D(num_filters, (3, 3), padding='same', activation='elu'))(input_tensor)\n",
    "    x = TimeDistributed(Conv2D(num_filters, (3, 3), padding='same', activation='elu'))(x)\n",
    "    return x\n",
    "\n",
    "def conv_lstm_block(input_tensor, num_filters):\n",
    "    x = Bidirectional(ConvLSTM2D(num_filters, (3, 3), padding='same', return_sequences=True))(input_tensor)\n",
    "    return x\n",
    "\n",
    "def unet_lstm(input_size=(256, 256, 3), n_sequences=10, n_filters=64, n_classes=4):\n",
    "    inputs = Input((n_sequences, *input_size))\n",
    "\n",
    "    # Downsample\n",
    "    c1 = conv_block(inputs, n_filters)\n",
    "    l1 = conv_lstm_block(c1, n_filters)\n",
    "    p1 = TimeDistributed(MaxPooling2D((2, 2)))(l1)\n",
    "\n",
    "    c2 = conv_block(p1, n_filters*2)\n",
    "    l2 = conv_lstm_block(c2, n_filters*2)\n",
    "    p2 = TimeDistributed(MaxPooling2D((2, 2)))(l2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = conv_block(p2, n_filters*4)\n",
    "    l3 =  conv_lstm_block(c3, n_filters)\n",
    "\n",
    "    # Upsample\n",
    "    u1 = TimeDistributed(UpSampling2D((2, 2)))(l3)\n",
    "    concat1 = concatenate([u1, l2], axis=4)\n",
    "    c4 = conv_block(concat1, n_filters*2)\n",
    "    l4 =  conv_lstm_block(c4, n_filters)\n",
    "\n",
    "    u2 = TimeDistributed(UpSampling2D((2, 2)))(l4)\n",
    "    concat2 = concatenate([u2, l1], axis=4)\n",
    "    c5 = conv_block(concat2, n_filters)\n",
    "    l5 = conv_lstm_block(c5, n_filters)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = TimeDistributed(Conv2D(n_classes, (1, 1), activation='softmax'))(l5)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = unet_lstm()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avBFHfMVbP6y",
    "outputId": "a059d260-d6df-468a-8424-56b2046ef4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sFLFtCmtbVOH",
    "outputId": "f2917eb0-9506-43be-e774-b20286af43e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Assuming your model is defined as 'model'\n",
    "model = unet_lstm()\n",
    "\n",
    "# Generate the plot\n",
    "plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True, expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: packaging in /home/voicelab/miniconda3/envs/tmpenv/lib/python3.11/site-packages (from tensorflow-addons) (23.2)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gf1OIID27Y4h",
    "outputId": "e132fa58-01a0-4fbf-e4b3-367d9d2ea970"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voicelab/miniconda3/envs/tmpenv/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Conv2D, MaxPooling2D, UpSampling2D, Activation, Dense, Concatenate\n",
    "from tensorflow_addons.layers import InstanceNormalization, FilterResponseNormalization\n",
    "\n",
    "\"\"\"\n",
    "GlottisNetV2e (Final version of GlottisNetV2)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "filters : int, optional\n",
    "    The number of filters in the first layer.\n",
    "    The subsequent layers have multiples of this filter number. Default: 16\n",
    "\n",
    "layers : int, optional\n",
    "    The number of encoding and decoding layers. Default: 4.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Keras Model\n",
    "    Using different decoders for prediction maps of anterior and posterior points and segmentation\n",
    "    U-Net structure in tensorflow.keras with 2 outputs\n",
    "    First output: Prediction maps of of anterior and posterior point (2 channels)\n",
    "    Second output: Segmentation map\"\"\"\n",
    "\n",
    "def Decoder(input_tensor, to_concat, name = 'decoder_', layers = 4, filters = 16):\n",
    "    x  = input_tensor\n",
    "\n",
    "    # Decoding path for prediction maps of anterior and posterior points\n",
    "    for step, filter_factor in enumerate(np.arange(layers)[::-1]):\n",
    "\n",
    "        # First convolution in layer followed by instance normalization and activation function\n",
    "        x = TimeDistributed(Conv2D(filters * (2 ** filter_factor), (3, 3), use_bias = False, padding = 'same', \\\n",
    "                         strides = 1, kernel_initializer = 'he_uniform', name = name + \"conv1\" + str(step))(x))\n",
    "        x = Bidirectional(ConvLSTM2D(filters, (3, 3), padding='same', return_sequences=True))(input_tensor)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # Umsampling and concatenation with results of encoding path\n",
    "        x = TimeDistributed(UpSampling2D(size = (2, 2), name = name + \"UpSampling\" + str(step))(x))\n",
    "        x = Concatenate()([x, to_concat[::-1][step]])\n",
    "\n",
    "        # Second convolution followed by batch normalization and activation function\n",
    "        x = TimeDistributed(Conv2D(filters * (2 ** filter_factor), (3, 3), use_bias = False, padding = 'same', \\\n",
    "                         strides = 1, kernel_initializer = 'he_uniform', name = name + \"conv2\" + str(step))(x))\n",
    "        x = Bidirectional(ConvLSTM2D(filters, (3, 3), padding='same', return_sequences=True))(input_tensor)\n",
    "\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "    # Last decoding step\n",
    "    x =TimeDistributed( Conv2D(filters * (2 ** filter_factor), (3, 3), use_bias = False, padding = 'same', \\\n",
    "                     strides = 1, kernel_initializer = 'he_uniform', name = name + \"last_decoding_step\" + str(step))(x))\n",
    "    x = InstanceNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def glottisnetV2_e(filters = 16, layers = 4, input_size = (512, 256, 1)):\n",
    "\n",
    "    in_layer = Input(input_size)\n",
    "    x = in_layer\n",
    "\n",
    "    # save layers to concat\n",
    "    to_concat = []\n",
    "\n",
    "    # Encoding path\n",
    "    for step in range(layers):\n",
    "\n",
    "        # Per layer apply two convolutions each followed by instance normalization and an activation function\n",
    "        x = TimeDistributed(Conv2D(filters * (2 ** step), (3, 3), use_bias = False, padding = 'same', strides = 1, \\\n",
    "                   kernel_initializer = 'he_uniform')(x))\n",
    "        x = Bidirectional(ConvLSTM2D(filters, (3, 3), padding='same', return_sequences=True))(input_tensor)\n",
    "\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = TimeDistributed(Conv2D(filters * (2 ** step), (3, 3), use_bias = False, padding = 'same', strides = 1, \\\n",
    "                   kernel_initializer = 'he_uniform')(x))\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # Append input to list to use it for concatenation in decoding path\n",
    "        to_concat.append(x)\n",
    "        x =TimeDistributed(MaxPooling2D(pool_size = (2, 2))(x))\n",
    "\n",
    "    # Last convolution (without MaxPooling) in latent space\n",
    "    x = TimeDistributed(Conv2D(filters * (2 ** (step + 1)), (3, 3), use_bias = False, padding = 'same', strides = 1, \\\n",
    "               kernel_initializer = 'he_uniform')(x))\n",
    "    x = InstanceNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Add two decoders for\n",
    "    upconv_seg = Decoder(x, to_concat, name = \"decode_seg\", layers= layers, filters = filters)\n",
    "    # Output maps\n",
    "    # 1x1 convolution to create 3 output maps (segmentation, prediction map of anterior point, prediction map of posterior point)\n",
    "    out_seg = TimeDistributed(Conv2D(1, (1, 1), use_bias = False, padding = \"same\", activation = 'sigmoid', strides = 1, \\\n",
    "                     kernel_initializer = 'glorot_uniform', name = 'seg')(upconv_seg))\n",
    "\n",
    "\n",
    "   # Create model\n",
    "    model = Model(in_layer, [out_ap, out_seg])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01N5O14Z04vj",
    "outputId": "9feb3d8a-99f0-442f-c84c-d48715cfb67b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 sequences.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import logging\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "from random import shuffle\n",
    "from glob import glob\n",
    "\n",
    "class SequenceDataProvider:\n",
    "    def __init__(self, data_dir, sequence_length, image_suffix, label_suffix, input_size, n_classes, shuffle_data=True, max_sequences=None):\n",
    "        self._data_dir = data_dir\n",
    "        self._sequence_length = sequence_length\n",
    "        self._image_suffix = image_suffix\n",
    "        self._label_suffix = label_suffix\n",
    "        self._input_size = input_size\n",
    "        self._n_classes = n_classes\n",
    "        self._shuffle_data = shuffle_data\n",
    "        self._max_sequences = max_sequences\n",
    "        self._sequences = self._load_sequences()\n",
    "\n",
    "    def dataset_length(self):\n",
    "        # Devuelve el número total de secuencias cargadas\n",
    "        return len(self._sequences)\n",
    "\n",
    "    def _load_sequences(self):\n",
    "        image_paths = sorted(glob(f\"{self._data_dir}/*{self._image_suffix}\"))\n",
    "        label_paths = sorted(glob(f\"{self._data_dir}/*{self._label_suffix}\"))\n",
    "\n",
    "        sequences = [\n",
    "            list(zip(image_paths[i:i+self._sequence_length], label_paths[i:i+self._sequence_length]))\n",
    "            for i in range(0, len(image_paths) - self._sequence_length + 1, self._sequence_length)\n",
    "        ]\n",
    "\n",
    "        if self._max_sequences:\n",
    "            sequences = sequences[:self._max_sequences]\n",
    "\n",
    "        if self._shuffle_data:\n",
    "            shuffle(sequences)\n",
    "        print(f\"Loaded {len(sequences)} sequences.\")\n",
    "        return sequences\n",
    "\n",
    "    def _process_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(self._input_size[:2])  # Assuming input_size is (height, width, channels)\n",
    "        return np.array(image)\n",
    "\n",
    "    def _process_label(self, label_path):\n",
    "        label = Image.open(label_path)\n",
    "        label = label.resize(self._input_size[:2])\n",
    "        label = np.array(label, dtype=np.int32)  # Asegúrate de que sea entero para usar como índices\n",
    "\n",
    "        # Convertir la etiqueta a formato one-hot\n",
    "        one_hot_label = np.eye(self._n_classes)[label]\n",
    "        return one_hot_label\n",
    "\n",
    "    def next_batch(self, batch_size=1):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for _ in range(batch_size):\n",
    "            if len(self._sequences) == 0:\n",
    "                self._sequences = self._load_sequences()\n",
    "\n",
    "            sequence = self._sequences.pop(0)\n",
    "            sequence_images = []\n",
    "            sequence_labels = []\n",
    "\n",
    "            for image_path, label_path in sequence:\n",
    "                image = self._process_image(image_path)\n",
    "                label = self._process_label(label_path)\n",
    "                sequence_images.append(image)\n",
    "                sequence_labels.append(label)\n",
    "\n",
    "            batch_images.append(sequence_images)\n",
    "            batch_labels.append(sequence_labels)\n",
    "        print(np.array(batch_labels).shape)\n",
    "\n",
    "        batch_images = np.array(batch_images).reshape(batch_size, self._sequence_length, *self._input_size)\n",
    "        # Asegúrate de ajustar el reshape de batch_labels para que coincida con las dimensiones esperadas por tu modelo\n",
    "        batch_labels = np.array(batch_labels).reshape(batch_size, self._sequence_length, *self._input_size[:2], self._n_classes)\n",
    "\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "# Asegúrate de definir input_size y n_classes acorde a tu modelo\n",
    "input_size = (256, 256, 3)  # Ejemplo de tamaño de entrada\n",
    "n_classes = 4  # Ejemplo de número de clases\n",
    "\n",
    "data_provider = SequenceDataProvider(\n",
    "    data_dir=\"/content/drive/MyDrive/Glottis/dataset/dataset/train\",\n",
    "    sequence_length=10,\n",
    "    image_suffix=\"_rgb.png\",\n",
    "    label_suffix=\"_mask.png\",\n",
    "    input_size=(256, 256, 3),\n",
    "    n_classes=4,\n",
    "    shuffle_data=True,\n",
    "    max_sequences=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VnRXFt1Y6mVX",
    "outputId": "f394560b-4593-483c-b5b3-b04b88cfa969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 256, 256, 4)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node model/bidirectional/backward_conv_lstm2d/TensorArrayV2Stack/TensorListStack defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-17-20559b432c89>\", line 15, in <cell line: 15>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py\", line 279, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py\", line 409, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\", line 556, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_conv_lstm.py\", line 509, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_conv_rnn.py\", line 327, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5177, in rnn\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5177, in <genexpr>\n\nOOM when allocating tensor with shape[10,2,256,256,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/bidirectional/backward_conv_lstm2d/TensorArrayV2Stack/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_17537]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-20559b432c89>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Entrenar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model.fit(x=train_generator,\n\u001b[0m\u001b[1;32m     16\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           epochs=epochs)\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node model/bidirectional/backward_conv_lstm2d/TensorArrayV2Stack/TensorListStack defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-17-20559b432c89>\", line 15, in <cell line: 15>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py\", line 279, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py\", line 409, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\", line 556, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_conv_lstm.py\", line 509, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_conv_rnn.py\", line 327, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5177, in rnn\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5177, in <genexpr>\n\nOOM when allocating tensor with shape[10,2,256,256,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/bidirectional/backward_conv_lstm2d/TensorArrayV2Stack/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_17537]"
     ]
    }
   ],
   "source": [
    "epochs = 10  # Número de épocas para el entrenamiento\n",
    "batch_size = 2  # Tamaño del lote\n",
    "def data_generator(batch_size):\n",
    "    while True:\n",
    "        batch_images, batch_labels = data_provider.next_batch(batch_size)\n",
    "        yield batch_images, batch_labels\n",
    "\n",
    "# Crear el generador\n",
    "train_generator = data_generator(batch_size)\n",
    "\n",
    "# Calcular el número de pasos por época\n",
    "steps_per_epoch = data_provider.dataset_length() // batch_size\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x=train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=epochs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
